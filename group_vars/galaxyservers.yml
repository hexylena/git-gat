# Galaxy
galaxy_create_user: true # False by default, as e.g. you might have a 'galaxy' user provided by LDAP or AD.
galaxy_separate_privileges: true # Best practices for security, configuration is owned by 'root' (or a different user) than the processes
galaxy_manage_paths: true # False by default as your administrator might e.g. have root_squash enabled on NFS. Here we can create the directories so it's fine.
galaxy_manage_cleanup: true
galaxy_layout: root-dir
galaxy_root: /srv/galaxy
galaxy_user: {name: "{{ galaxy_user_name }}", shell: /bin/bash}
galaxy_commit_id: release_23.0
galaxy_force_checkout: true
miniconda_prefix: "{{ galaxy_tool_dependency_dir }}/_conda"
miniconda_version: 4.12.0
miniconda_channels: ['conda-forge', 'defaults']

# Galaxy Job Configuration
galaxy_job_config:
  runners:
    local_runner:
      load: galaxy.jobs.runners.local:LocalJobRunner
      workers: 4
    slurm:
      load: galaxy.jobs.runners.slurm:SlurmJobRunner
      drmaa_library_path: /usr/lib/slurm-drmaa/lib/libdrmaa.so.1
    pulsar_runner:
      load: galaxy.jobs.runners.pulsar:PulsarMQJobRunner
      amqp_url: "pyamqp://pulsar_au:{{ vault_rabbitmq_password_vhost }}@localhost:5671/{{ rabbitmq_vhosts[0] }}?ssl=1"
      amqp_acknowledge: true
      amqp_ack_republish_time: 1200
      amqp_consumer_timeout: 2
      amqp_publish_retry: true
      amqp_publish_retry_max_retries: 60
      galaxy_url: "https://{{ inventory_hostname }}"
      manager: _default_
  handling:
    assign: ['db-skip-locked']
  execution:
    default: tpv_dispatcher
    environments:
      local_env:
        runner: local_runner
        tmp_dir: true
      tpv_dispatcher:
        runner: dynamic
        type: python
        function: map_tool_to_destination
        rules_module: tpv.rules
        tpv_config_files:
          - https://raw.githubusercontent.com/galaxyproject/tpv-shared-database/main/tools.yml
          - "{{ tpv_config_dir }}/tpv_rules_local.yml"
  resources:
    default: default
    groups:
      default: []
      testing: [cores, time]
  tools:
    - class: local # these special tools that aren't parameterized for remote execution - expression tools, upload, etc
      environment: local_env
    - id: testing
      environment: tpv_dispatcher
      resources: testing

galaxy_config:
  galaxy:
    # Branding
    brand: Mars ðŸš€
    logo_src: "https://training.galaxyproject.org/training-material/topics/admin/tutorials/customization/images/logo.png"
    themes_config_file: "{{ galaxy_config_dir }}/themes.yml"
    # Main Configuration
    admin_users:
    - admin@example.org
    database_connection: "postgresql:///{{ galaxy_db_name }}?host=/var/run/postgresql"
    file_path: /data/datasets
    job_working_directory: /data/jobs
    object_store_store_by: uuid
    id_secret: "{{ vault_id_secret }}"
    job_config: "{{ galaxy_job_config }}" # Use the variable we defined above
    job_resource_params_file: "{{ galaxy_config_dir }}/job_resource_params_conf.xml"
    # SQL Performance
    slow_query_log_threshold: 5
    enable_per_request_sql_debugging: true
    # File serving Performance
    nginx_x_accel_redirect_base: /_x_accel_redirect
    # Automation / Ease of Use / User-facing features
    watch_job_rules: 'auto'
    allow_path_paste: true
    enable_quotas: true
    allow_user_deletion: true
    show_welcome_with_login: true
    expose_user_name: true
    expose_dataset_path: true
    expose_potentially_sensitive_job_metrics: true
    # NFS workarounds
    retry_job_output_collection: 3
    # Debugging
    cleanup_job: onsuccess
    allow_user_impersonation: true
    # Tool security
    outputs_to_working_directory: true
    new_user_dataset_access_role_default_private: true # Make datasets private by default
    # TUS
    galaxy_infrastructure_url: "https://{{ inventory_hostname }}"
    tus_upload_store: "{{ galaxy_tus_upload_store }}"
    # CVMFS
    tool_data_table_config_path: /cvmfs/data.galaxyproject.org/byhand/location/tool_data_table_conf.xml,/cvmfs/data.galaxyproject.org/managed/location/tool_data_table_conf.xml
    # Tool Dependencies
    dependency_resolvers_config_file: "{{ galaxy_config_dir }}/dependency_resolvers_conf.xml"
    containers_resolvers_config_file: "{{ galaxy_config_dir }}/container_resolvers_conf.yml"
    # Data Library Directories
    library_import_dir: /libraries/admin
    user_library_import_dir: /libraries/user
    # Celery
    amqp_internal_connection: "pyamqp://galaxy:{{ vault_rabbitmq_password_galaxy }}@localhost:5671/galaxy_internal?ssl=1"
    celery_conf:
      result_backend: "redis://localhost:6379/0"
    enable_celery_tasks: true
    # Monitoring
    statsd_host: localhost
    statsd_influxdb: true
    sentry_dsn: "{{ vault_galaxy_sentry_dsn }}"
    sentry_traces_sample_rate: 0.5
    error_report_file: "{{ galaxy_config_dir }}/error_reports_file.yml"
  gravity:
    process_manager: systemd
    galaxy_root: "{{ galaxy_root }}/server"
    galaxy_user: "{{ galaxy_user_name }}"
    virtualenv: "{{ galaxy_venv_dir }}"
    gunicorn:
      # listening options
      bind: "unix:{{ galaxy_mutable_config_dir }}/gunicorn.sock"
      # performance options
      workers: 2
      # Other options that will be passed to gunicorn
      # This permits setting of 'secure' headers like REMOTE_USER (and friends)
      # https://docs.gunicorn.org/en/stable/settings.html#forwarded-allow-ips
      extra_args: '--forwarded-allow-ips="*"'
      # This lets Gunicorn start Galaxy completely before forking which is faster.
      # https://docs.gunicorn.org/en/stable/settings.html#preload-app
      preload: true
    celery:
      concurrency: 2
      enable_beat: true
      enable: true
      queues: celery,galaxy.internal,galaxy.external
      pool: threads
      memory_limit: 2
      loglevel: DEBUG
    tusd:
      enable: true
      tusd_path: /usr/local/sbin/tusd
      upload_dir: "{{ galaxy_tus_upload_store }}"
    handlers:
      handler:
        processes: 2
        pools:
          - job-handlers
          - workflow-schedulers
    reports:
      enable: true
      url_prefix: /reports
      bind: "unix:{{ galaxy_mutable_config_dir }}/reports.sock"
      config_file: "{{ galaxy_config_dir }}/reports.yml"

galaxy_job_config_file: "{{ galaxy_config_dir }}/galaxy.yml"

galaxy_config_files_public:
  - src: files/galaxy/welcome.html
    dest: "{{ galaxy_mutable_config_dir }}/welcome.html"

galaxy_config_files:
  - src: files/galaxy/themes.yml
    dest: "{{ galaxy_config.galaxy.themes_config_file }}"
  - src: files/galaxy/config/tpv_rules_local.yml
    dest: "{{ tpv_mutable_dir }}/tpv_rules_local.yml"
  - src: files/galaxy/config/error_reports.yml
    dest: "{{ galaxy_config.galaxy.error_report_file }}"

galaxy_config_templates:
  - src: templates/galaxy/config/container_resolvers_conf.yml.j2
    dest: "{{ galaxy_config.galaxy.containers_resolvers_config_file }}"
  - src: templates/galaxy/config/dependency_resolvers_conf.xml
    dest: "{{ galaxy_config.galaxy.dependency_resolvers_config_file }}"
  - src: templates/galaxy/config/job_resource_params_conf.xml.j2
    dest: "{{ galaxy_config.galaxy.job_resource_params_file }}"
  - src: templates/galaxy/config/reports.yml
    dest: "{{ galaxy_config.gravity.reports.config_file }}"

galaxy_extra_dirs:
  - /data
  - "{{ galaxy_config_dir }}/{{ tpv_config_dir_name }}"

galaxy_extra_privsep_dirs:
  - "{{ tpv_mutable_dir }}"
tpv_privsep: true

galaxy_local_tools:
- testing.xml
- job_properties.xml

# Certbot
certbot_auto_renew_hour: "{{ 23 |random(seed=inventory_hostname)  }}"
certbot_auto_renew_minute: "{{ 59 |random(seed=inventory_hostname)  }}"
certbot_auth_method: --webroot
certbot_install_method: virtualenv
certbot_auto_renew: yes
certbot_auto_renew_user: root
certbot_environment: staging
certbot_well_known_root: /srv/nginx/_well-known_root
certbot_share_key_users:
  - www-data
  - proftpd
certbot_share_key_ids:
  - "999:999"
certbot_post_renewal: |
    systemctl restart nginx || true
    docker restart rabbit_hole || true
    systemctl restart proftpd || true
certbot_domains:
 - "{{ inventory_hostname }}"
certbot_agree_tos: --agree-tos

# NGINX
nginx_selinux_allow_local_connections: true
nginx_servers:
  - redirect-ssl
nginx_ssl_servers:
  - galaxy
  - sentry
nginx_enable_default_server: false
nginx_conf_http:
  client_max_body_size: 1g
  # gzip: "on" # This is enabled by default in Ubuntu, and the duplicate directive will cause a crash.
  gzip_proxied: "any"
  gzip_static: "on"   # The ngx_http_gzip_static_module module allows sending precompressed files with the ".gz" filename extension instead of regular files.
  gzip_vary: "on"
  gzip_min_length: 128
  gzip_comp_level: 6  # Tradeoff of better compression for slightly more CPU time.
  gzip_types: |
      text/plain
      text/css
      text/xml
      text/javascript
      application/javascript
      application/x-javascript
      application/json
      application/xml
      application/xml+rss
      application/xhtml+xml
      application/x-font-ttf
      application/x-font-opentype
      image/png
      image/svg+xml
      image/x-icon
nginx_ssl_role: usegalaxy_eu.certbot
nginx_conf_ssl_certificate: /etc/ssl/certs/fullchain.pem
nginx_conf_ssl_certificate_key: /etc/ssl/user/privkey-www-data.pem

# Slurm
slurm_roles: ['controller', 'exec'] # Which roles should the machine play? exec are execution hosts.
slurm_nodes:
- name: localhost # Name of our host
  CPUs: 2         # Here you would need to figure out how many cores your machine has. For this training we will use 2 but in real life, look at `htop` or similar.
slurm_config:
  SlurmdParameters: config_overrides   # Ignore errors if the host actually has cores != 2
  SelectType: select/cons_res
  SelectTypeParameters: CR_CPU_Memory  # Allocate individual cores/memory instead of entire node

#Install pip docker package for ansible
pip_install_packages:
  - name: docker
# RabbitMQ
rabbitmq_container:
  name: rabbit_hole
  image: rabbitmq:3.11-management
  hostname: "{{ inventory_hostname }}"

rabbitmq_plugins:
  - rabbitmq_management

rabbitmq_config:
  listeners:
    tcp: none
  ssl_listeners:
    default: 5671
  ssl_options:
    verify: verify_peer
    cacertfile: /etc/ssl/certs/fullchain.pem
    certfile: /etc/ssl/certs/cert.pem
    keyfile: /etc/ssl/user/privkey-999:999.pem
    fail_if_no_peer_cert: 'false'
  management_agent:
    disable_metrics_collector: "false"
  management:
    disable_stats: 'false'
  consumer_timeout: 21600000 # 6 hours in milliseconds

rabbitmq_vhosts:
  - /pulsar/pulsar_au
  - galaxy_internal

rabbitmq_users:
  - user: admin
    password: "{{ vault_rabbitmq_admin_password }}"
    tags: administrator
    vhost: /
  - user: pulsar_au
    password: "{{ vault_rabbitmq_password_vhost }}"
    vhost: /pulsar/pulsar_au
  - user: galaxy
    password: "{{ vault_rabbitmq_password_galaxy }}"
    vhost: galaxy_internal
  - user: flower
    password: "{{ vault_rabbitmq_password_flower }}"
    tags: administrator
    vhost: galaxy_internal

# TUS
galaxy_tusd_port: 1080
galaxy_tus_upload_store: /data/tus

#Redis
galaxy_additional_venv_packages:
  - redis

# Flower
flower_python_version: python3
flower_app_dir: "{{ galaxy_root }}"
flower_python_path: "{{ galaxy_root }}/server/lib"
flower_venv_dir: "{{ galaxy_venv_dir }}"
flower_app_name: galaxy.celery
flower_db_file: "{{ galaxy_root }}/var/flower.db"
flower_persistent: true
flower_broker_api: "https://flower:{{ vault_rabbitmq_password_flower }}@localhost:5671/api/"
flower_broker_url: "amqp://flower:{{ vault_rabbitmq_password_flower }}@localhost:5671/galaxy_internal?ssl=true"
flower_proxy_prefix: /flower

flower_ui_users:
  - name: admin
    password: "{{ vault_flower_user_password}}"

flower_environment_variables:
  GALAXY_CONFIG_FILE: "{{ galaxy_config_file }}"

# Telegraf
telegraf_plugins_extra:
  listen_galaxy_routes:
    plugin: "statsd"
    config:
      - service_address = ":8125"
      - metric_separator = "."
      - allowed_pending_messages = 10000
  monitor_galaxy_queue:
    plugin: "exec"
    config:
      - commands = ["/usr/bin/env PGDATABASE=galaxy /usr/local/bin/gxadmin iquery queue-overview --short-tool-id"]
      - timeout = "10s"
      - data_format = "influx"
      - interval = "15s"

# TIaaS setup
tiaas_dir: /srv/tiaas
tiaas_admin_user: admin
tiaas_admin_pass: changeme
